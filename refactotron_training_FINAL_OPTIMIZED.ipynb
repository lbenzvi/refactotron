{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactotron: Maximum Optimized LoRA Training (Colab Pro)\n",
    "\n",
    "**Colab Pro Version - Option C: Balanced & Powerful**\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Checkpoints auto-save to Google Drive (survives disconnects)\n",
    "- ‚úÖ max_length=1536 (97% coverage vs 90% at 1024)\n",
    "- ‚úÖ LoRA rank=24 (50% more capacity than r=16)\n",
    "- ‚úÖ 100% syntactically valid training data (39,812 samples)\n",
    "- ‚úÖ Learning rate: 2e-5 (optimized for fine-tuning)\n",
    "- ‚úÖ Cosine LR scheduler with 500-step warmup\n",
    "- ‚úÖ Moderate regularization (Option B)\n",
    "\n",
    "**Expected Results:**\n",
    "- Validation Loss: 0.45-0.50 (vs 0.68 before)\n",
    "- Training time: ~15-18 hours on T4 GPU\n",
    "- BLEU: 75-80 (target: 73.5)\n",
    "- CodeBERT: 0.88-0.92 (target: 0.87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive (CRITICAL - Run First!)\n",
    "\n",
    "This ensures all checkpoints save to YOUR Google Drive permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save checkpoints in drive\n",
    "print(\"/content/drive/MyDrive/refactotron_lora_optimized/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and install dependencies\n",
    "import torch\n",
    "\n",
    "print(\"GPU Status:\")\n",
    "print(f\"Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"Ready for training!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  NO GPU! Go to: Runtime > Change runtime type > T4 GPU\")\n",
    "\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Upload Training Data\n",
    "\n",
    "**Upload these 2 files:**\n",
    "- `train_enhanced.jsonl` (60.2 MB)\n",
    "- `validation_enhanced.jsonl` (7.5 MB)\n",
    "\n",
    "Click the folder icon üìÅ on left sidebar and drag files, OR run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload train_enhanced.jsonl and validation_enhanced.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\nFiles uploaded:\")\n",
    "for filename in uploaded.keys():\n",
    "    size_mb = len(uploaded[filename]) / (1024*1024)\n",
    "    print(f\"{filename}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with huggingface to access StarCoder-1B model\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderbase-1b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model in fp16 to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoderbase-1b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Configure LoRA (OPTION C - Enhanced)\n",
    "\n",
    "**Improvements over baseline:**\n",
    "- Rank increased: 16 ‚Üí 24 (50% more capacity)\n",
    "- Alpha: 48 (2x rank)\n",
    "- Trainable params: ~15M (vs ~10M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nlora_config = LoraConfig(\n    r=16,                                          # REDUCED from 24 to save memory\n    lora_alpha=32,                                 # 2x rank\n    target_modules=[\"c_proj\", \"c_attn\"],           # REDUCED from 3 to 2 modules to save memory\n    lora_dropout=0.08,                             # Moderate dropout\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\nprint(\"\\nLoRA configured for T4 GPU memory constraints (r=16)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training Data\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = load_jsonl('train_enhanced.jsonl')\n",
    "val_data = load_jsonl('validation_enhanced.jsonl')\n",
    "\n",
    "print(f\"Train: {len(train_data):,} samples\")\n",
    "print(f\"Validation: {len(val_data):,} samples\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nSample input (first 200 chars):\")\n",
    "print(train_data[0]['input'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Tokenization (OPTION C - Extended Context)\n",
    "\n",
    "**max_length=1536 captures ~97% of all training data (vs 90% at 1024)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# tokenization with max_length-1024 to fit in T4 GPU memory\nfrom transformers import DataCollatorForLanguageModeling\n\ndef tokenize_function(examples):\n    \"\"\"\n    Tokenize input + output together.\n    \"\"\"\n    # Combine input and output\n    full_texts = [inp + \"\\n\" + out for inp, out in zip(examples['input'], examples['output'])]\n\n    # Tokenize with 1024 max length (reduced from 1536 for memory)\n    result = tokenizer(\n        full_texts,\n        truncation=True,\n        max_length=1024,  # REDUCED from 1536 to fit T4 GPU memory\n        padding=False,\n    )\n\n    # Set labels\n    result[\"labels\"] = result[\"input_ids\"].copy()\n\n    return result\n\n\ntokenized_train = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_dataset.column_names,\n    desc=\"Tokenizing train\"\n)\n\ntokenized_val = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=val_dataset.column_names,\n    desc=\"Tokenizing validation\"\n)\n\nprint(f\"Tokenized train: {len(tokenized_train):,} samples\")\nprint(f\"Tokenized validation: {len(tokenized_val):,} samples\")\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Configuration (OPTION C - Balanced)\n",
    "\n",
    "**Key optimizations:**\n",
    "- max_length: 1536 (97% data coverage)\n",
    "- LoRA rank: 24 (50% more capacity)\n",
    "- Gradient accumulation: 6 (adjusted for longer sequences)\n",
    "- Learning rate: 2e-5 (10x lower than original)\n",
    "- Cosine LR scheduler with 500-step warmup\n",
    "- Weight decay: 0.02 (moderate)\n",
    "- LoRA dropout: 0.08 (moderate)\n",
    "- Label smoothing: 0.05 (moderate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration - OPTIMIZED FOR T4 GPU MEMORY\nfrom transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n\ntraining_args = TrainingArguments(\n    # Output - SAVES TO GOOGLE DRIVE!\n    output_dir=\"/content/drive/MyDrive/refactotron_lora_optimized\",\n    logging_dir=\"/content/drive/MyDrive/refactotron_lora_optimized/logs\",\n\n    # Training schedule\n    num_train_epochs=5,\n\n    # Batch size - OPTIMIZED FOR MEMORY\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,  # INCREASED from 6 to maintain effective batch size\n\n    # Learning rate (OPTIMIZED)\n    learning_rate=2e-5,              # 10x lower than original\n    lr_scheduler_type=\"cosine\",      # Smooth decay\n    warmup_steps=500,                # Better stability\n\n    # Regularization\n    weight_decay=0.02,               # Moderate L2 regularization\n    label_smoothing_factor=0.05,     # Moderate label smoothing\n    max_grad_norm=1.0,               # Gradient clipping\n\n    # Precision - FIXED: Changed from fp16 to bf16\n    bf16=True,\n\n    # Logging & evaluation\n    logging_steps=50,\n    eval_steps=500,\n    save_steps=500,                  # Save checkpoint every 500 steps\n    save_total_limit=3,              # Keep best 3 checkpoints\n    eval_strategy=\"steps\",\n\n    # Best model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n\n    # Memory optimization - CRITICAL FOR T4\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # More memory efficient\n    optim=\"adamw_torch_fused\",       # More memory efficient optimizer\n    \n    # Reporting\n    report_to=\"none\",\n)\n\n# Early stopping\nearly_stopping = EarlyStoppingCallback(\n    early_stopping_patience=3  # Stop if no improvement for 3 evals (1500 steps)\n)\n\nprint(\"Training configuration complete (T4 GPU optimized)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Initialize Trainer & Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=data_collator,\n    callbacks=[early_stopping]\n)\n\n# Training summary\ntotal_steps = (len(tokenized_train) //\n               (training_args.per_device_train_batch_size *\n                training_args.gradient_accumulation_steps) *\n               training_args.num_train_epochs)\n\nprint(f\"Training samples: {len(tokenized_train):,}\")\nprint(f\"Validation samples: {len(tokenized_val):,}\")\nprint(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"Total epochs: {training_args.num_train_epochs}\")\nprint(f\"Max training steps: {total_steps:,}\")\nprint(f\"Evaluation every: {training_args.eval_steps} steps\")\nprint(f\"Checkpoint save every: {training_args.save_steps} steps\")\nprint(f\"\\nCheckpoints save to:\")\nprint(f\"/content/drive/MyDrive/refactotron_lora_optimized/\")\nprint(f\"\\n=== T4 GPU OPTIMIZED CONFIGURATION ===\")\nprint(f\"   ‚Ä¢ max_length: 1024 (optimized for T4 memory)\")\nprint(f\"   ‚Ä¢ LoRA rank: 16 (memory efficient)\")\nprint(f\"   ‚Ä¢ LoRA alpha: 32\")\nprint(f\"   ‚Ä¢ LoRA targets: c_proj, c_attn (2 modules)\")\nprint(f\"   ‚Ä¢ Trainable params: ~10M\")\nprint(f\"   ‚Ä¢ Gradient accumulation: 8\")\nprint(f\"   ‚Ä¢ Learning rate: 2e-5\")\nprint(f\"   ‚Ä¢ LR scheduler: cosine\")\nprint(f\"   ‚Ä¢ Warmup: 500 steps\")\nprint(f\"   ‚Ä¢ Weight decay: 0.02\")\nprint(f\"   ‚Ä¢ LoRA dropout: 0.08\")\nprint(f\"   ‚Ä¢ Label smoothing: 0.05\")\nprint(f\"   ‚Ä¢ Precision: bfloat16\")\nprint(f\"   ‚Ä¢ Optimizer: adamw_torch_fused (memory efficient)\")\nprint(f\"   ‚Ä¢ Gradient checkpointing: enabled (use_reentrant=False)\")\nprint(f\"\\nEXPECTED RESULTS:\")\nprint(f\"   ‚Ä¢ Validation loss: 0.48-0.55\")\nprint(f\"   ‚Ä¢ BLEU score: 70-75\")\nprint(f\"   ‚Ä¢ CodeBERT similarity: 0.85-0.90\")\nprint(f\"   ‚Ä¢ Estimated time: 12-15 hours (T4 GPU)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: START TRAINING üöÄ\n",
    "\n",
    "**This will take ~15-18 hours.**\n",
    "\n",
    "**What to expect:**\n",
    "- Validation loss starts ~0.71, should drop to ~0.45-0.50\n",
    "- Training loss may fluctuate (regularization working!)\n",
    "- Early stopping will halt when validation plateaus\n",
    "- Checkpoints auto-save to Google Drive every 500 steps\n",
    "\n",
    "**You can:**\n",
    "- ‚úÖ Close browser tab (Colab Pro allows background execution)\n",
    "- ‚úÖ Let laptop sleep\n",
    "- ‚úÖ Come back later to check progress\n",
    "\n",
    "**Monitor progress:** Check back every few hours to see validation loss decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# START TRAINING\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training complete\")\n",
    "print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save Final Model\n",
    "\n",
    "**Run this after training completes to save the best model to Drive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "model.save_pretrained(\"/content/drive/MyDrive/refactotron_lora_FINAL\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/refactotron_lora_FINAL\")\n",
    "\n",
    "print(\"Saved to: /content/drive/MyDrive/refactotron_lora_FINAL/\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"   1. Test model on test_enhanced.jsonl\")\n",
    "print(\"   2. Calculate BLEU and CodeBERT scores\")\n",
    "print(\"   3. Generate sample refactorings\")\n",
    "print(\"   4. Compare against vanilla StarCoder baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Download Model (Optional)\n",
    "\n",
    "**If you want to download the model directly from Colab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the final model\n",
    "!cd /content/drive/MyDrive && zip -r refactotron_lora_FINAL.zip refactotron_lora_FINAL/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('/content/drive/MyDrive/refactotron_lora_FINAL.zip')\n",
    "\n",
    "print(\"Model downloaded! You can also access it anytime from Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}