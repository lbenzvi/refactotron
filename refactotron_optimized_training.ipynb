{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactotron: Optimized LoRA Training\n",
    "\n",
    "**Improvements over previous version:**\n",
    "- ‚úÖ 100% syntactically valid training data (vs 93% broken)\n",
    "- ‚úÖ Learning rate: 2e-5 (down from 2e-4)\n",
    "- ‚úÖ Cosine LR scheduler with proper warmup\n",
    "- ‚úÖ Weight decay regularization\n",
    "- ‚úÖ Expanded LoRA target modules\n",
    "\n",
    "**Expected Results:**\n",
    "- Validation Loss: 0.55-0.60 (vs 0.68 before)\n",
    "- BLEU Score: 70-73 (target: 73.5)\n",
    "- CodeBERT: 0.85-0.87 (target: 0.87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  GPU Status:\n",
      "   Available: False\n",
      "   ‚ö†Ô∏è  NO GPU! Go to: Runtime > Change runtime type > T4 GPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"üñ•Ô∏è  GPU Status:\")\n",
    "print(f\"   Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  NO GPU! Go to: Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Training Data\n",
    "\n",
    "**Click the folder icon on the left sidebar and upload:**\n",
    "- `train.jsonl` (7,943 samples)\n",
    "- `validation.jsonl` (992 samples)\n",
    "\n",
    "Or run the cell below to upload via file picker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload train.jsonl and validation.jsonl\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your HuggingFace token when prompted\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "print(\"üì• Loading model and tokenizer...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderbase-1b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model in fp16 to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoderbase-1b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Base model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA (Optimized)\n",
    "\n",
    "**Key improvements:**\n",
    "- Added `c_fc` to target modules (MLP layers)\n",
    "- Light dropout (0.05) for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è  Configuring LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                      # Rank\n",
    "    lora_alpha=32,                             # Scaling factor (2x rank)\n",
    "    target_modules=[\"c_proj\", \"c_attn\", \"c_fc\"],  # üî• Added c_fc for MLP layers\n",
    "    lora_dropout=0.05,                         # Light dropout\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"‚úÖ LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print(\"üìÇ Loading training data...\")\n",
    "\n",
    "# Load the data\n",
    "train_data = load_jsonl('train.jsonl')\n",
    "val_data = load_jsonl('validation.jsonl')\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_data)} samples\")\n",
    "print(f\"‚úÖ Validation: {len(val_data)} samples\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "# Preview one sample\n",
    "print(\"\\nüìù Sample training example:\")\n",
    "print(f\"Input (first 200 chars):\\n{train_data[0]['input'][:200]}...\")\n",
    "print(f\"\\nOutput (first 200 chars):\\n{train_data[0]['output'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize input + output together.\n",
    "    The model will learn to predict the output given the input.\n",
    "    \"\"\"\n",
    "    # Combine input and output\n",
    "    full_texts = [inp + \"\\n\" + out for inp, out in zip(examples['input'], examples['output'])]\n",
    "\n",
    "    # Tokenize\n",
    "    result = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    # Set labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"üîÑ Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úÖ Validation: {len(tokenized_val)} samples\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Configuration (OPTIMIZED)\n",
    "\n",
    "**üî• Key Optimizations:**\n",
    "1. **Learning rate: 2e-5** (down from 2e-4) - 10x lower for fine-tuning\n",
    "2. **Cosine LR scheduler** - smooth decay instead of constant\n",
    "3. **Warmup: 500 steps** (up from 100) - more stable training\n",
    "4. **Weight decay: 0.01** - L2 regularization\n",
    "5. **Early stopping: patience=3** - prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuring training (OPTIMIZED)...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./refactotron_lora_optimized\",\n",
    "    logging_dir=\"./logs\",\n",
    "\n",
    "    # Training schedule\n",
    "    num_train_epochs=5,\n",
    "\n",
    "    # Batch size\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,              # Effective batch size = 8\n",
    "\n",
    "    # Learning rate (üî• OPTIMIZED)\n",
    "    learning_rate=2e-5,                         # üî• DOWN from 2e-4 (10x lower!)\n",
    "    lr_scheduler_type=\"cosine\",                 # üî• ADDED cosine decay\n",
    "    warmup_steps=500,                           # üî• UP from 100\n",
    "\n",
    "    # Regularization (üî• OPTIMIZED)\n",
    "    weight_decay=0.01,                          # üî• ADDED weight decay\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Precision\n",
    "    fp16=True,\n",
    "\n",
    "    # Logging & evaluation\n",
    "    logging_steps=50,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "\n",
    "    # Best model selection\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Reporting\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Initializing trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Training summary\n",
    "total_steps = (len(tokenized_train) //\n",
    "               (training_args.per_device_train_batch_size *\n",
    "                training_args.gradient_accumulation_steps) *\n",
    "               training_args.num_train_epochs)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total training samples: {len(tokenized_train)}\")\n",
    "print(f\"Validation samples: {len(tokenized_val)}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Evaluation every: {training_args.eval_steps} steps\")\n",
    "print(f\"\\nüî• OPTIMIZATIONS APPLIED:\")\n",
    "print(f\"  ‚Ä¢ Learning rate: 2e-5 (was 2e-4)\")\n",
    "print(f\"  ‚Ä¢ LR scheduler: cosine (was none)\")\n",
    "print(f\"  ‚Ä¢ Warmup steps: 500 (was 100)\")\n",
    "print(f\"  ‚Ä¢ Weight decay: 0.01 (was 0)\")\n",
    "print(f\"  ‚Ä¢ LoRA targets: c_proj, c_attn, c_fc (added c_fc)\")\n",
    "print(f\"\\nüìà EXPECTED RESULTS:\")\n",
    "print(f\"  ‚Ä¢ Validation loss: 0.55-0.60 (vs 0.68 before)\")\n",
    "print(f\"  ‚Ä¢ BLEU score: 70-73 (vs target 73.5)\")\n",
    "print(f\"  ‚Ä¢ CodeBERT similarity: 0.85-0.87 (vs target 0.87)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Start Training üöÄ\n",
    "\n",
    "**This will take 2-3 hours depending on GPU.**\n",
    "\n",
    "Monitor the validation loss - it should:\n",
    "- Decrease steadily from ~0.7 to ~0.55-0.60\n",
    "- Stop early if no improvement for 3 evaluations\n",
    "- NOT plateau at 0.68 like before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\\n\")\n",
    "\n",
    "# START TRAINING\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving final model...\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "model.save_pretrained(\"./refactotron_lora_final\")\n",
    "tokenizer.save_pretrained(\"./refactotron_lora_final\")\n",
    "\n",
    "print(\"‚úÖ Model saved to ./refactotron_lora_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Download Model (Optional)\n",
    "\n",
    "Download the trained model to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model folder\n",
    "!zip -r refactotron_lora_final.zip refactotron_lora_final/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('refactotron_lora_final.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Inference (Optional)\n",
    "\n",
    "Test the model on a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = load_jsonl('test.jsonl') if 'test.jsonl' in !ls else val_data\n",
    "\n",
    "# Pick a random sample\n",
    "import random\n",
    "sample = random.choice(test_data)\n",
    "\n",
    "print(\"üìù INPUT (Degraded Code):\")\n",
    "print(\"=\" * 70)\n",
    "input_text = sample['input']\n",
    "print(input_text)\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract just the refactored part\n",
    "refactored = generated.split('### Refactored code:')[1].strip() if '### Refactored code:' in generated else generated\n",
    "\n",
    "print(\"\\nü§ñ MODEL OUTPUT (Refactored):\")\n",
    "print(\"=\" * 70)\n",
    "print(refactored[:500])\n",
    "\n",
    "print(\"\\n‚úÖ EXPECTED OUTPUT:\")\n",
    "print(\"=\" * 70)\n",
    "print(sample['output'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Next Steps\n",
    "\n",
    "**For full evaluation:**\n",
    "1. Load test.jsonl\n",
    "2. Generate refactored code for all test samples\n",
    "3. Calculate BLEU score\n",
    "4. Calculate CodeBERT similarity\n",
    "\n",
    "**Expected Results:**\n",
    "- BLEU: 70-73 (target: 73.5)\n",
    "- CodeBERT: 0.85-0.87 (target: 0.87)\n",
    "\n",
    "If you hit these targets, you've successfully completed the project! üéØ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
