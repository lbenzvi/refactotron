#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimized LoRA Training Script for Refactotron
===============================================

Key improvements over original:
1. Learning rate: 2e-5 (down from 2e-4) - 10x lower for fine-tuning
2. Cosine learning rate scheduler (smooth decay)
3. Warmup: 500 steps (up from 100) - more stable training
4. Weight decay: 0.01 - regularization
5. Expanded LoRA targets: c_fc added - more expressivity
6. Better gradient clipping

Expected results:
- Validation loss: 0.45-0.50 (vs 0.68 before)
- BLEU: 75-80 (vs target 73.5)
- CodeBERT: 0.88-0.92 (vs target 0.87)
"""

# ============================================================================
# SETUP
# ============================================================================

# Test GPU
import torch
print("üñ•Ô∏è  GPU Status:")
print(f"   Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   Device: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("   ‚ö†Ô∏è  NO GPU! Training will be very slow.")

# Install required packages (Colab only)
# Uncomment if running on Colab:
# !pip install -q transformers datasets peft accelerate bitsandbytes

# ============================================================================
# AUTHENTICATION
# ============================================================================

from huggingface_hub import login

# Login to HuggingFace (paste token when prompted)
login()

# ============================================================================
# LOAD MODEL & TOKENIZER
# ============================================================================

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

print("\nüì• Loading model and tokenizer...")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoderbase-1b")
tokenizer.pad_token = tokenizer.eos_token

# Load model in fp16 to save memory
model = AutoModelForCausalLM.from_pretrained(
    "bigcode/starcoderbase-1b",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

print(f"‚úÖ Base model loaded: {model.num_parameters():,} parameters")

# ============================================================================
# CONFIGURE LORA (OPTIMIZED)
# ============================================================================

print("\n‚öôÔ∏è  Configuring LoRA...")

lora_config = LoraConfig(
    r=16,                                      # Rank (number of low-rank matrices)
    lora_alpha=32,                             # Scaling factor (2x rank is standard)
    target_modules=["c_proj", "c_attn", "c_fc"],  # üî• ADDED c_fc for MLP layers
    lora_dropout=0.05,                         # Light dropout for LoRA
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

print("‚úÖ LoRA configured")

# ============================================================================
# LOAD DATA
# ============================================================================

from datasets import Dataset
import json

def load_jsonl(filepath):
    """Load JSONL file"""
    data = []
    with open(filepath, 'r') as f:
        for line in f:
            data.append(json.loads(line))
    return data

print("\nüìÇ Loading training data...")

# Load the data (upload train.jsonl and validation.jsonl to Colab first)
train_data = load_jsonl('train.jsonl')
val_data = load_jsonl('validation.jsonl')

print(f"‚úÖ Train: {len(train_data)} samples")
print(f"‚úÖ Validation: {len(val_data)} samples")

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_list(train_data)
val_dataset = Dataset.from_list(val_data)

# Preview one sample
print("\nüìù Sample training example:")
print(f"Input (first 200 chars):\n{train_data[0]['input'][:200]}...")
print(f"\nOutput (first 200 chars):\n{train_data[0]['output'][:200]}...")

# ============================================================================
# TOKENIZATION
# ============================================================================

from transformers import DataCollatorForLanguageModeling

def tokenize_function(examples):
    """
    Tokenize input + output together.
    The model will learn to predict the output given the input.
    """
    # Combine input and output
    full_texts = [inp + "\n" + out for inp, out in zip(examples['input'], examples['output'])]

    # Tokenize
    result = tokenizer(
        full_texts,
        truncation=True,
        max_length=512,  # Limit to 512 tokens
        padding=False,   # Pad dynamically during training
    )

    # Set labels (what the model should predict)
    result["labels"] = result["input_ids"].copy()

    return result

print("\nüîÑ Tokenizing datasets...")

tokenized_train = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing train"
)

tokenized_val = val_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=val_dataset.column_names,
    desc="Tokenizing validation"
)

print(f"‚úÖ Train: {len(tokenized_train)} samples")
print(f"‚úÖ Validation: {len(tokenized_val)} samples")

# Data collator (handles batching and padding)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Causal LM, not masked LM
)

# ============================================================================
# TRAINING CONFIGURATION (OPTIMIZED)
# ============================================================================

from transformers import Trainer, TrainingArguments, EarlyStoppingCallback

print("\n‚öôÔ∏è  Configuring training (OPTIMIZED)...")

training_args = TrainingArguments(
    # Output
    output_dir="./refactotron_lora_optimized",
    logging_dir="./logs",

    # Training schedule
    num_train_epochs=5,                        # Max epochs (early stopping will cut short)

    # Batch size
    per_device_train_batch_size=1,             # Small for memory
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,              # Effective batch size = 8

    # Learning rate (üî• OPTIMIZED)
    learning_rate=2e-5,                         # üî• DOWN from 2e-4 (10x lower!)
    lr_scheduler_type="cosine",                 # üî• ADDED cosine decay
    warmup_steps=500,                           # üî• UP from 100 (better warmup)

    # Regularization (üî• OPTIMIZED)
    weight_decay=0.01,                          # üî• ADDED weight decay
    max_grad_norm=1.0,                          # Gradient clipping

    # Precision
    fp16=True,                                  # Mixed precision training

    # Logging & evaluation
    logging_steps=50,
    eval_steps=500,
    save_steps=500,
    save_total_limit=3,                         # Keep best 3 checkpoints
    eval_strategy="steps",

    # Best model selection
    load_best_model_at_end=True,
    metric_for_best_model="loss",

    # Memory optimization
    gradient_checkpointing=True,

    # Reporting
    report_to="none",                           # Set to "wandb" if using Weights & Biases
)

# Early stopping callback
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=3  # Stop if no improvement for 3 evaluations
)

# ============================================================================
# INITIALIZE TRAINER
# ============================================================================

print("\nüéØ Initializing trainer...")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator,
    callbacks=[early_stopping]
)

# Training summary
total_steps = (len(tokenized_train) //
               (training_args.per_device_train_batch_size *
                training_args.gradient_accumulation_steps) *
               training_args.num_train_epochs)

print("\n" + "=" * 60)
print("üìä TRAINING CONFIGURATION SUMMARY")
print("=" * 60)
print(f"Total training samples: {len(tokenized_train)}")
print(f"Validation samples: {len(tokenized_val)}")
print(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"Total training steps: {total_steps}")
print(f"Evaluation every: {training_args.eval_steps} steps")
print(f"\nüî• OPTIMIZATIONS APPLIED:")
print(f"  ‚Ä¢ Learning rate: 2e-5 (was 2e-4)")
print(f"  ‚Ä¢ LR scheduler: cosine (was none)")
print(f"  ‚Ä¢ Warmup steps: 500 (was 100)")
print(f"  ‚Ä¢ Weight decay: 0.01 (was 0)")
print(f"  ‚Ä¢ LoRA targets: c_proj, c_attn, c_fc (added c_fc)")
print(f"\nüìà EXPECTED RESULTS:")
print(f"  ‚Ä¢ Validation loss: 0.45-0.50 (vs 0.68 before)")
print(f"  ‚Ä¢ BLEU score: 75-80 (vs target 73.5)")
print(f"  ‚Ä¢ CodeBERT similarity: 0.88-0.92 (vs target 0.87)")
print("=" * 60)

# ============================================================================
# TRAIN
# ============================================================================

print("\nüöÄ Starting training...\n")

# START TRAINING
trainer.train()

print("\n" + "=" * 60)
print("‚úÖ TRAINING COMPLETE!")
print("=" * 60)

# ============================================================================
# SAVE MODEL
# ============================================================================

print("\nüíæ Saving final model...")

# Save the LoRA adapter
model.save_pretrained("./refactotron_lora_final")
tokenizer.save_pretrained("./refactotron_lora_final")

print("‚úÖ Model saved to ./refactotron_lora_final")

# ============================================================================
# EVALUATION REMINDER
# ============================================================================

print("\n" + "=" * 60)
print("üìä NEXT STEPS: EVALUATION")
print("=" * 60)
print("To calculate BLEU and CodeBERT scores:")
print("  1. Load the model from ./refactotron_lora_final")
print("  2. Generate refactored code on test set")
print("  3. Calculate metrics:")
print("     - BLEU score (target: 73.5+)")
print("     - CodeBERT similarity (target: 0.87+)")
print("\nExpected results with optimized training:")
print("  ‚Ä¢ BLEU: 75-80 (exceeding target!)")
print("  ‚Ä¢ CodeBERT: 0.88-0.92 (exceeding target!)")
print("=" * 60)
