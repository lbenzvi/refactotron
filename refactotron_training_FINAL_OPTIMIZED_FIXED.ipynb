{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactotron: Fixed Training (Proper Label Masking)\n",
    "\n",
    "**CRITICAL FIX:** This version properly masks input tokens so loss is only computed on output tokens.\n",
    "\n",
    "**Expected Results:**\n",
    "- Validation Loss: 0.48-0.55 (should decrease from ~0.71)\n",
    "- Training time: ~12-15 hours on T4 GPU\n",
    "- BLEU score: 70-75\n",
    "- CodeBERT similarity: 0.85-0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"/content/drive/MyDrive/refactotron_lora_optimized/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GPU Status:\")\n",
    "print(f\"Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"Ready for training!\")\n",
    "else:\n",
    "    print(\"NO GPU! Go to: Runtime > Change runtime type > T4 GPU\")\n",
    "\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Upload Training Data\n",
    "\n",
    "**IMPORTANT: Upload the ENHANCED files (larger files):**\n",
    "- `train_enhanced.jsonl` (~60 MB) - NOT train.jsonl\n",
    "- `validation_enhanced.jsonl` (~7.5 MB) - NOT validation.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload train_enhanced.jsonl and validation_enhanced.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\nFiles uploaded:\")\n",
    "for filename in uploaded.keys():\n",
    "    size_mb = len(uploaded[filename]) / (1024*1024)\n",
    "    print(f\"{filename}: {size_mb:.1f} MB\")\n",
    "    \n",
    "# Verify correct files\n",
    "if 'train_enhanced.jsonl' not in uploaded:\n",
    "    print(\"\\n⚠️  WARNING: You need to upload 'train_enhanced.jsonl' (not 'train.jsonl')!\")\n",
    "if 'validation_enhanced.jsonl' not in uploaded:\n",
    "    print(\"\\n⚠️  WARNING: You need to upload 'validation_enhanced.jsonl' (not 'validation.jsonl')!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderbase-1b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoderbase-1b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_proj\", \"c_attn\"],\n",
    "    lora_dropout=0.08,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"\\nLoRA configured (r=16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "train_data = load_jsonl('train_enhanced.jsonl')\n",
    "val_data = load_jsonl('validation_enhanced.jsonl')\n",
    "\n",
    "print(f\"Train: {len(train_data):,} samples\")\n",
    "print(f\"Validation: {len(val_data):,} samples\")\n",
    "\n",
    "# VERIFY DATA SIZE\n",
    "if len(train_data) < 30000:\n",
    "    print(f\"\\n⚠️  WARNING: Only {len(train_data)} training samples!\")\n",
    "    print(\"Expected ~39,000+ samples. Did you upload the wrong file?\")\n",
    "else:\n",
    "    print(\"✓ Data size looks good!\")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(\"\\nSample input (first 200 chars):\")\n",
    "print(train_data[0]['input'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Tokenization with PROPER LABEL MASKING\n",
    "\n",
    "**CRITICAL FIX:** This version masks input tokens so loss is only computed on output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize input + output, but MASK input tokens in labels.\n",
    "    Loss is only computed on the output portion.\n",
    "    \"\"\"\n",
    "    # Tokenize inputs and outputs separately first\n",
    "    input_encodings = tokenizer(\n",
    "        examples['input'],\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    output_encodings = tokenizer(\n",
    "        examples['output'],\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Combine input + output\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for inp_ids, out_ids in zip(input_encodings['input_ids'], output_encodings['input_ids']):\n",
    "        # Combine sequences\n",
    "        combined = inp_ids + out_ids\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(combined) > 1024:\n",
    "            combined = combined[:1024]\n",
    "        \n",
    "        # Create labels: -100 for input tokens (ignored), actual IDs for output tokens\n",
    "        labels = [-100] * len(inp_ids) + out_ids\n",
    "        if len(labels) > 1024:\n",
    "            labels = labels[:1024]\n",
    "        \n",
    "        input_ids_list.append(combined)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_list,\n",
    "        'labels': labels_list\n",
    "    }\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized train: {len(tokenized_train):,} samples\")\n",
    "print(f\"Tokenized validation: {len(tokenized_val):,} samples\")\n",
    "\n",
    "# Data collator with padding\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/refactotron_lora_optimized\",\n",
    "    logging_dir=\"/content/drive/MyDrive/refactotron_lora_optimized/logs\",\n",
    "    \n",
    "    num_train_epochs=5,\n",
    "    \n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    weight_decay=0.02,\n",
    "    label_smoothing_factor=0.05,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    bf16=True,\n",
    "    \n",
    "    logging_steps=50,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3\n",
    ")\n",
    "\n",
    "print(\"Training configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "total_steps = (len(tokenized_train) //\n",
    "               (training_args.per_device_train_batch_size *\n",
    "                training_args.gradient_accumulation_steps) *\n",
    "               training_args.num_train_epochs)\n",
    "\n",
    "print(f\"Training samples: {len(tokenized_train):,}\")\n",
    "print(f\"Validation samples: {len(tokenized_val):,}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Max training steps: {total_steps:,}\")\n",
    "print(f\"\\n=== EXPECTED RESULTS (WITH PROPER LABEL MASKING) ===\")\n",
    "print(f\"   • Initial validation loss: ~0.68-0.71\")\n",
    "print(f\"   • Target validation loss: 0.48-0.55\")\n",
    "print(f\"   • Training loss should be similar to validation loss (not 10x higher!)\")\n",
    "print(f\"   • Estimated time: 12-15 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: START TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training complete\")\n",
    "print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/refactotron_lora_FINAL\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/refactotron_lora_FINAL\")\n",
    "\n",
    "print(\"Saved to: /content/drive/MyDrive/refactotron_lora_FINAL/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
